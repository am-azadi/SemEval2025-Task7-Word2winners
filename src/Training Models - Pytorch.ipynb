{"cells":[{"cell_type":"markdown","metadata":{"id":"0MPyaQH7V94y"},"source":["# initializations"]},{"cell_type":"code","source":["!pip install transformers bitsandbytes accelerate"],"metadata":{"id":"bMrZxGujJR1G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzqM03cIOKII"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import random\n","import pickle\n","import torch\n","import json\n","import ast\n","import os"]},{"cell_type":"markdown","metadata":{"id":"ewWl3klCZmU9"},"source":["# loading data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2480,"status":"ok","timestamp":1740129689595,"user":{"displayName":"امیر محمد آزادی","userId":"12043806229481158638"},"user_tz":-210},"id":"25IHWfXUU6YL","outputId":"13cbd852-362d-43fc-fe6a-a2fcdf442352"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1740129689595,"user":{"displayName":"امیر محمد آزادی","userId":"12043806229481158638"},"user_tz":-210},"id":"LFb_Ab31U9HZ","outputId":"371f1367-6ed4-4dd8-8948-9794ed10a938"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1iZ2XHgIpDSkxPjihIgMQ_KPj766HC2So/Research/SemEval 2025: Task 7\n"]}],"source":["# %cd /content/drive/MyDrive/University/Research/SemEval 2025: Task 7\n","%cd /content/drive/MyDrive/Research/SemEval 2025: Task 7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4reJwdpUKSU"},"outputs":[],"source":["import sys\n","sys.path.append('./src')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpqYDAbeULAm"},"outputs":[],"source":["import utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSEDfzW5aTtH"},"outputs":[],"source":["parse_col = lambda s: ast.literal_eval(s.replace('\\n', '\\\\n')) if s else s\n","\n","\n","fact_checks_df = pd.read_csv('./data/cleaned data/fact_checks.csv').fillna('').set_index('fact_check_id')\n","\n","for col in ['claim', 'title']:\n","    fact_checks_df[col] = fact_checks_df[col].apply(parse_col)\n","\n","\n","posts_df = pd.read_csv('./data/cleaned data/posts.csv').fillna('').set_index('post_id')\n","\n","mapping_df = pd.read_csv('./data/original data/pairs.csv')\n","\n","with open('./data/original data/tasks.json', 'r') as file:\n","    tasks = json.load(file)"]},{"cell_type":"markdown","metadata":{"id":"DJVwpJGBct61"},"source":["# Models"]},{"cell_type":"markdown","metadata":{"id":"eSqjMAVaXXeF"},"source":["## gte-multilingual-base"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WOztQFr13PSS"},"outputs":[],"source":["from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":9858,"status":"ok","timestamp":1739799535397,"user":{"displayName":"AmirMohammad Azadi","userId":"02121840807222239367"},"user_tz":-210},"id":"tZNmitNO3Wgj","outputId":"f56c32df-e51d-4ce6-8128-3012ac0615e4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: {'classifier.bias', 'classifier.weight'}\n","- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=\"float16\",  # Use float16 for better accuracy\n","    bnb_4bit_use_double_quant=True,    # Improves compression efficiency\n","    bnb_4bit_quant_type=\"nf4\"          # NF4 works best for LLMs\n",")\n","\n","model_name = \"Alibaba-NLP/gte-multilingual-base\"\n","model = AutoModel.from_pretrained(\n","                                    model_name,\n","                                    quantization_config= quantization_config,\n","                                    trust_remote_code=True,\n","                                    device_map=\"auto\"\n","                                  )\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ovkon-sN34m0"},"outputs":[],"source":["def get_embeddings(ids, data, batch_size = 16):\n","\n","    embeddings = {}\n","\n","    for i in tqdm(range(0, len(data), batch_size), desc=\"Processing Batches\"):\n","        batch_data = data[i:i + batch_size]\n","        batch_id = ids[i:i + batch_size]\n","\n","        # Batch process embeddings\n","        inputs = tokenizer(batch_data, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","        with torch.no_grad():\n","            emb = model(**inputs).last_hidden_state[:, 0, :]  # CLS token embedding\n","\n","        embeddings.update(dict(zip(batch_id, emb.cpu().numpy())))\n","\n","    return embeddings"]},{"cell_type":"markdown","metadata":{"id":"SyxBvHMKdXW2"},"source":["# Building train dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kd0vbkK6dZU4"},"outputs":[],"source":["def get_fact_checks(post_id):\n","    FCs = mapping_df[mapping_df['post_id'] == post_id]['fact_check_id'].to_list()\n","    return fact_checks_df.loc[FCs].index.to_list()\n","\n","def get_negative_samples(FCs, nearest_FCs, k):\n","    result = []\n","    for element in nearest_FCs:\n","        if element not in FCs and len(result) < k:\n","            result.append(element)\n","    return result\n","\n","def get_samples(posts_ids, top_indices_ids):\n","    positive_samples, negative_samples = [], []\n","    for i, p in enumerate(posts_ids):\n","        FCs = get_fact_checks(p)\n","\n","        positive_samples.append(FCs)\n","        negative_samples.append(get_negative_samples(FCs, top_indices_ids[i], 3))\n","\n","    return positive_samples, negative_samples\n","\n","def sort_indices_by_similarity(similarities, top_indices):\n","    sorted_top_indices = []\n","    for i in range(similarities.shape[0]):\n","      # Get similarities for current post\n","        current_similarities = similarities[i, :]\n","        # Get the top indices for current post\n","        current_indices = top_indices[i]\n","\n","        # Create a dictionary of indices and their corresponding similarities\n","        idx_similarity = {idx: sim for idx, sim in zip(current_indices, current_similarities[np.array(current_indices)])}\n","\n","        # Sort indices by similarity in descending order\n","        sorted_indices = sorted(idx_similarity, key=idx_similarity.get, reverse=True)\n","\n","        sorted_top_indices.append(sorted_indices)\n","\n","    return sorted_top_indices\n","\n","def get_positive_and_negative_samples(posts, fact_checks_embeddings):\n","    posts_embedding = get_embeddings(posts.index.to_list(), posts['content'].to_list(), batch_size = 16)\n","\n","    similarities = cosine_similarity(list(posts_embedding.values()), fact_checks_embeddings['embedding'].to_list())\n","    # similarities = cosine_similarity(list(posts_embedding.values()), list(fc_emb.values()))\n","\n","    nearest = np.argpartition(similarities, -10, axis=1)[:, -10:]\n","    nearest = sort_indices_by_similarity(similarities, nearest)\n","    top_indices = [[fact_checks_embeddings.iloc[idx].name for idx in sublist] for sublist in nearest]\n","    # top_indices = [[list(fact_checks_embeddings.keys())[idx] for idx in sublist] for sublist in nearest]\n","\n","    positive_samples, negative_samples = get_samples(posts.index, top_indices)\n","\n","    return positive_samples, negative_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8dcDTkMnAHM"},"outputs":[],"source":["fact_checks_embeddings = utils.load_fact_checks_embeddings(\"gte-multilingual-base\")"]},{"cell_type":"markdown","metadata":{"id":"jQk9tYC1imXR"},"source":["## manual testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4RW6Vowxdce_"},"outputs":[],"source":["lang = 'deu'\n","fc = fact_checks_df.loc[tasks['monolingual'][lang]['fact_checks']]\n","# fc = fact_checks_df.loc[tasks['crosslingual']['fact_checks']]\n","\n","fc_emb = fact_checks_embeddings.loc[fc.index]\n","# fc_emb = get_embeddings(fc.index.to_list(), fc['claim'].apply(lambda x: x[0]).to_list(), batch_size = 16)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xbv7yRMYdegy"},"outputs":[],"source":["posts = posts_df.loc[tasks['monolingual'][lang]['posts_train']].head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":372,"status":"ok","timestamp":1739611741073,"user":{"displayName":"AmirMohammad Azadi","userId":"02121840807222239367"},"user_tz":-210},"id":"YId19KXLq3lw","outputId":"58ce0785-4c47-42a6-e3f7-df0ce24993c9"},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing Batches: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\n"]},{"data":{"text/plain":["([[87108],\n","  [150241],\n","  [98619],\n","  [118585],\n","  [77239],\n","  [45291],\n","  [25311, 45723],\n","  [53337],\n","  [111002],\n","  [58104]],\n"," [[89646, 53380, 45567],\n","  [23340, 151813, 151814],\n","  [62469, 71566, 44050],\n","  [118586, 151811, 49895],\n","  [44056, 49733, 77238],\n","  [71307, 70016, 118142],\n","  [45735, 44050, 42628],\n","  [26764, 41249, 40340],\n","  [68256, 44739, 118811],\n","  [44031, 45243, 63987]])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["get_positive_and_negative_samples(posts, fc_emb)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1739605215392,"user":{"displayName":"AmirMohammad Azadi","userId":"02121840807222239367"},"user_tz":-210},"id":"fFoTatrHiW4B","outputId":"f2c309cb-8809-411a-ffca-0b6a90c61457"},"outputs":[{"data":{"text/plain":["[[87108],\n"," [150241],\n"," [98619],\n"," [118585],\n"," [77239],\n"," [45291],\n"," [25311, 45723],\n"," [53337],\n"," [111002],\n"," [58104]]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["positive_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1739605215392,"user":{"displayName":"AmirMohammad Azadi","userId":"02121840807222239367"},"user_tz":-210},"id":"DfAuLrCciYUN","outputId":"74214a9f-de05-467a-82f8-db0a3b7eb306"},"outputs":[{"data":{"text/plain":["[[89646, 53380, 45567],\n"," [23340, 151813, 151814],\n"," [62469, 71566, 44050],\n"," [118586, 151811, 49895],\n"," [44056, 49733, 77238],\n"," [71307, 70016, 118142],\n"," [45735, 44050, 42628],\n"," [26764, 41249, 40340],\n"," [68256, 44739, 118811],\n"," [44031, 45243, 63987]]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["negative_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":8,"status":"ok","timestamp":1739605215393,"user":{"displayName":"AmirMohammad Azadi","userId":"02121840807222239367"},"user_tz":-210},"id":"8ZwRgxR6iZcA","outputId":"824a7a2f-d361-4cee-ce61-dbaacbe376d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["content: ! Brazen vaccination fake by Markus Söder! It's really unbelievable how bold Top politicians such as Markus Söder kidding us. On Instagram does Söder busy advertising for vaccination But if you look closely, you can see you that he can't be injected at all. The lid is still on the needle. You can see how much those who want to vaccinate you, the Trust vaccines! markus.soeder TBE ...\n","\n"," positive samples:\n","\tMarkus Söder faked his vaccination.\n","\n"," negative samples:\n","\tWith a combination of vaccination date photos, mood is being made on social media against Bavaria's Prime Minister Markus Söder. Some use the collage (archived here) to question how often the CSU boss got vaccinated. A connection to the corona vaccination is often made. Some make (archived here) - probably jokingly - a connection between the head of government and the shortage of preparations in Germany.\n","\tIt is suggested that Markus Söder did not have anything injected with a current corona vaccination because the cap was still on the needle.\n","\tThis photo collage transfers Markus Söder as a vaccination actor\n","============================================================================================================\n","content: \"The need will die force people to bend!\" That's the meanest sentence ever from a man was spoken to by German politicians!\n","\n"," positive samples:\n","\tWolfgang Schäuble said: \"Necessity will force people to bow down!\"\n","\n"," negative samples:\n","\tFormer Chancellor Schmidt said: \"If you don't like Germany, you're welcome to go.\"\n","\tQuotes from AfD politicians and AfD employees\n","\tQuotes from AfD politicians and AfD employees\n","============================================================================================================\n","content: \"A PEOPLE THAT CORRUPT, TRAITOR, STACKER AND THIEVES CHOOSE IS NO VICTIM BUT AN ACCOMPLICE.\" - George Orwell\n","\n"," positive samples:\n","\tOrwell has labeled the electorate as the accomplices of corrupt traitors\n","\n"," negative samples:\n","\tGeorge Orwell wrote these words about prohibitions\n","\tIn his famous book «1984», the British writer George Orwell describes an imaginary totalitarian society. Formulations are regularly attributed to the book with which comparisons to the situation during the corona pandemic are constructed. Currently circulating on Facebook (archived here): \"Everything but work is forbidden: walking on the streets and squares is forbidden. Having fun, singing and dancing is forbidden. Meeting people and coming together is forbidden.\"\n","\tThe British writer George Orwell is said to have commented on a society's relationship to truth. A photo of him is spreading on social media, next to which one can read: \"The further a society moves away from the truth, the more it will hate those who speak it.\" Below the alleged quote is: George Orwell (archived here).\n","============================================================================================================\n","content: \"You can't be vice chancellor and interior minister one be republic if against these people an investigation to be led.\" Sebastian Kurz about HC Strache and Herbert Kickl after the Ibiza video became known.\n","\n"," positive samples:\n","\tSebastian Kurz described investigations against himself as the reason for leaving office.\n","\n"," negative samples:\n","\tSebastian Kurz said that in relation to Heinz-Christian Strache.\n","\tQuote about Vice Chancellor comes from Sebastian Kurz.\n","\tA website claims that the public prosecutor's office in Berlin is investigating the Austrian ex-Chancellor Sebastian Kurz. The allegations were \"murder, sexual abuse of children and other crimes,\" it said. Even an alleged file number is included: \"8 St 221 / 19 b\".\n","============================================================================================================\n","content: \"We cannot say, with Omikron we hear now on. Omicron will the pandemic too so don't quit even if it's actually so would be what we prevent will. Germany will help cope forever indebted to the corona pandemic Being Bion Tech, that's why it's him Gratitude owed that compulsory vaccination is enforced.\" Federal Health Minister Karl Lauterbach f in the - COVID-19 Information Center ||| < X\n","\n"," positive samples:\n","\tKarl Lauterbach said: \"Germany will forever be indebted to Biontech for dealing with the pandemic, so it is also a debt of gratitude that compulsory vaccination is being enforced.\"\n","\n"," negative samples:\n","\tThe German Health Minister Karl Lauterbach (SPD) says he wants to prevent the Omicron variant from ending the corona pandemic.\n","\tA quotation image shows a statement by Lauterbach on compulsory vaccination out of eternal gratitude to Biontech.\n","\tKarl Lauterbach said the federal government would prevent the end of the pandemic.\n","============================================================================================================\n","content: +43 650 2111311 Forwarded THIS INFORMATION ARE FIRST HAND, FROM A DIPLOMA NURSE, ~Mark THAT I PERSONALLY KNOW!!! LG. OKSANA. A good friend reported: \"Now a friend was with me, their granddaughter nurse in the private clinic in Döbling. They already have the first Vaccination damage in private patients get delivered. A fit Man around 60, can't move more, don't know anything anymore, can't do his bodily functions control more... The doctors there have informed their employees that they do not respond to any vaccination of the personnel exist. Too many side effects. Also in Tulln im Hospital is supposed to be patients with massive give side effects. The man the grandson's nurse is there.\" news 19:43 @\n","\n"," positive samples:\n","\tThe hospitals in Döbing and Tulln treat patients with severe vaccination damage.\n","\n"," negative samples:\n","\tPeople with vaccination damage or massive vaccination side effects were admitted to the private clinic in Döbling and to the hospital in Tulln. In Döbling, doctors have informed their employees that they do not insist on vaccinating the staff due to the many side effects.\n","\tPosts critical of vaccination are not uncommon on social media. For example, claims are often made that people with vaccine damage or side effects are treated in hospitals. For example, a Facebook post (archived here) states that “the first vaccination damage to private patients has already been admitted” to the private clinic in Döbling. Doctors had informed their employees that they would not insist on vaccination of the staff. \"Too many side effects,\" they say. There would also be patients with \"massive side effects\" in the hospital in Tulln.\n","\tSerious side effects up to and including death, vaccinations that go against recommendations and an overburdened rescue service: A voice message distributed on Facebook contains a whole series of claims about complications after corona vaccinations in nursing homes in Düren, North Rhine-Westphalia (archived here). What's wrong with the descriptions?\n","============================================================================================================\n","content: ,,The reason why people to silence to be brought is not because they lie but because they die speak truth. if people lie, theirs can own words against they are applied. But if they telling the truth, there is no other object mean than violence.\" Theodor Fontane (1819 to 1889)\n","\n"," positive samples:\n","\tAlleged quote from Theodor Fontane after which he said that people would be \"silenced\" if they told \"the truth\".\n","\tThis quote comes from the writer Theodor Fontane.\n","\n"," negative samples:\n","\tDieter Hallervorden said: \"The biggest problem in the history of mankind is that the people who know the truth don't open their mouths. And you just can't shut up those who don't know anything.\"\n","\tThe British writer George Orwell is said to have commented on a society's relationship to truth. A photo of him is spreading on social media, next to which one can read: \"The further a society moves away from the truth, the more it will hate those who speak it.\" Below the alleged quote is: George Orwell (archived here).\n","\tThe quote comes from Theodor Fontane\n","============================================================================================================\n","content: \"It's more likely to die from the vaccine than from Corona\" Prof. Dr. Peter - McCullough at the annual meeting the American Association of Phys asicians and surgeons\" in the USA. -He sums up five from his experience Essential Truths About Co- vid-19 together: Genetically Engineered Vaccines Baylor University Medical Cen are neither effective nor ter in Dallas, Texas. He is know- secure. The obligation to vaccinate must be approved by a consultant who is one of the fall. There are (as of Oct. 2021) five most-publishing medicines 19,000 vaccination deaths in the USA, of which researchers in the USA heard 50% within 48 hours and has hundreds of releases post-vaccination and 80% internal. Dozens on Covid alone. He half a week. Furthermore, there are reports of irresponsibility there 31,000 reported permanent official Impikampa Vaccination damage, with a huge gne based on official documents Unreported, the factor he can the CDC (RKI of the USA). find it approximately between 10-100 despite numerous severe side effects lie. All of this is unacceptable. effects and death stalls none Security surveillance in place, Kin who would without need of danger severe myocarditis exposed and early treatment lungs of people with good medication suppressed. see H n. e Covid-19 can only transmit be when symptoms of disease are present. There is not a single case of one asymptomatic transmission Tests on asymptomatic people sons will neither of the FDA nor any other agency required. Since there is no asympto- matic transmission there are School closures and masks duty pointless Who symptoms has, just stay home, like always. Naturally acquired immunity is sturdy and durable, it is once and done Covid-19 is a drug treatable disease, esp ten from day one From:  more likely-on-the-vaccine-too die-as-of-covid-why-can- Prof. Dr. Peter McCullough is in-the-vaccination-definitely-continue/ ternist. Cardiologist, epidemiologist and Deputy Chief Physician at Dieter Böhme, Gera This division may be followed by civil war Reason has ruined our world ate Instead, arbitrariness reigns ind hate masks, isolation, fear the promise of freedom if they follow the rulers It's in the Gera newspaper\n","\n"," positive samples:\n","\tYou are more likely to die from the vaccine than from Corona.\n","\n"," negative samples:\n","\tArticle in the \"Geraer Zeitung\" claims: It is more likely to die from the vaccine than from a corona infection.\n","\tCovid-19 vaccines kill more people than they save\n","\tCorona vaccinations are more dangerous than the disease Covid-19\n","============================================================================================================\n","content: ... and it IS like 1939, everyone notices it except the German Michel The Germans are unable to get out of to learn history. Just like they did in 1939 Hitler to war followed, so would they Today in her Blindness, Merkel in follow the abyss. Vladimir Vladimirovich Putin\n","\n"," positive samples:\n","\tPutin compares the following of German Hitler supporters with the German population during Angela Merkel's tenure\n","\n"," negative samples:\n","\tA supposed quote from Winston Churchill, who was Prime Minister of Great Britain during much of the Second World War, has been circulating on the Internet. Churchill is said to have said in the late 1930s: \"You must be clear that this war is not against Hitler or National Socialism, but against the strength of the German people, which one wants to smash forever, regardless of whether it is in the hands of Hitler or a Jesuit priest.\"\n","\tGermany is an occupied country.\n","\tAn alleged quote from Winston Churchill (1874-1965) has been circulating on the internet for years. The former British Prime Minister is said to have said: \"Germany's unforgivable crime before the Second World War was the attempt to detach its economic power from the world trading system and to create its own exchange system from which world finance could no longer earn money.\" The words are mainly spread in English, but also in German.\n","============================================================================================================\n","content: 09/04/17 17:03. Uhc. 5.206 Greens call for a ban on barbecues - only for Germans In order to save the climate, Green Party leader Anton Hofreiter demands now a ban on grilling. \"The polar ice caps are melting, Antarctica breaks, polar bears die out. We are the first generation feeling the effects of the climate crisis. But we are too the last generation that can do something about it.\" The Greens in Berlin were already successful 3 years ago Patio heaters, the radiant heaters for patios, are prohibited. Asked about the fact that many fellow Anatolian citizens also grilled court rider \"However, an exception can be made for fellow Turkish citizens without a German passport because barbecuing is part of the culture in parts of Europe and it has to be to respect. It's enough if the Germans stop grilling. That's a first important contribution to the preservation of the polar ice caps.\"\n","\n"," positive samples:\n","\tGreen politician Anton Hofreiter called for a ban on barbecues for Germans\n","\n"," negative samples:\n","\tThe chairman of the Greens parliamentary group, Anton Hofreiter, called for a ban on barbecues \"only for Germans\".\n","\tGreens say \"no thanks\" to bans on cruises, fireworks, barbecues, motoring, single-family homes, meat and air travel.\n","\tGreen: No candles indoors\n","============================================================================================================\n"]}],"source":["for i in range(len(posts)):\n","    print(f\"content: {posts.iloc[i]['eng_content']}\")\n","    print(f\"\\n positive samples:\")\n","    for j in positive_samples[i]:\n","        print(f\"\\t{fc.loc[j]['claim'][1]}\")\n","    print(f\"\\n negative samples:\")\n","    for j in negative_samples[i]:\n","        print(f\"\\t{fc.loc[j]['claim'][1]}\")\n","    print(\"============================================================================================================\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1739605969179,"user":{"displayName":"AmirMohammad Azadi","userId":"02121840807222239367"},"user_tz":-210},"id":"PVGg8QzSjJns","outputId":"47561b7a-3cfc-45a7-81bd-eb77df409c54"},"outputs":[{"data":{"text/plain":["[0, 9, 10, 25, 52, 95, 96, 98, 108, 120]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["posts.index.to_list()"]},{"cell_type":"markdown","metadata":{"id":"v22Irn-WoFKg"},"source":["## building positive and negative samples for posts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQ4yuk5ApXh5"},"outputs":[],"source":["positive_samples_indices, negative_samples_indices = {}, {}"]},{"cell_type":"markdown","metadata":{"id":"RDO-7EpepwWw"},"source":["### monoligual posts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":875652,"status":"ok","timestamp":1739785557311,"user":{"displayName":"AmirMohammad Azadi","userId":"02121840807222239367"},"user_tz":-210},"id":"76e8A1qZmIZR","outputId":"a61af0e8-e041-4d8a-f60f-0d79a87ca57c"},"outputs":[{"name":"stdout","output_type":"stream","text":["lang: fra, posts: 1596, fc: 4355\n"]},{"name":"stderr","output_type":"stream","text":["Processing Batches: 100%|██████████| 100/100 [01:31<00:00,  1.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["lang: spa, posts: 5628, fc: 14082\n"]},{"name":"stderr","output_type":"stream","text":["Processing Batches: 100%|██████████| 352/352 [04:51<00:00,  1.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["lang: eng, posts: 4351, fc: 85734\n"]},{"name":"stderr","output_type":"stream","text":["Processing Batches: 100%|██████████| 272/272 [03:27<00:00,  1.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["lang: por, posts: 2571, fc: 21569\n"]},{"name":"stderr","output_type":"stream","text":["Processing Batches: 100%|██████████| 161/161 [01:33<00:00,  1.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["lang: tha, posts: 465, fc: 382\n"]},{"name":"stderr","output_type":"stream","text":["Processing Batches: 100%|██████████| 30/30 [00:37<00:00,  1.24s/it]\n"]},{"name":"stdout","output_type":"stream","text":["lang: deu, posts: 667, fc: 4996\n"]},{"name":"stderr","output_type":"stream","text":["Processing Batches: 100%|██████████| 42/42 [00:58<00:00,  1.40s/it]\n"]},{"name":"stdout","output_type":"stream","text":["lang: msa, posts: 1062, fc: 8424\n"]},{"name":"stderr","output_type":"stream","text":["Processing Batches: 100%|██████████| 67/67 [00:43<00:00,  1.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["lang: ara, posts: 676, fc: 14201\n"]},{"name":"stderr","output_type":"stream","text":["Processing Batches: 100%|██████████| 43/43 [00:17<00:00,  2.42it/s]\n"]}],"source":["for lang in tasks['monolingual'].keys():\n","    posts = posts_df.loc[tasks['monolingual'][lang]['posts_train']]\n","    fc_embeddings = fact_checks_embeddings.loc[tasks['monolingual'][lang]['fact_checks']]\n","\n","    print(f\"lang: {lang}, posts: { len(posts) }, fc: { len(fc_embeddings) }\")\n","\n","    positive_samples, negative_samples = get_positive_and_negative_samples(posts, fc_embeddings)\n","\n","    positive_samples_indices.update(dict(zip(posts.index.to_list(), positive_samples)))\n","    negative_samples_indices.update(dict(zip(posts.index.to_list(), negative_samples)))"]},{"cell_type":"markdown","metadata":{"id":"7EXS7R7Jpy3k"},"source":["### crosslingual posts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":325948,"status":"ok","timestamp":1739785883255,"user":{"displayName":"AmirMohammad Azadi","userId":"02121840807222239367"},"user_tz":-210},"id":"vZZ3Kk7Zob_V","outputId":"6bb79a9b-d61f-4936-eacd-79e88e946b13"},"outputs":[{"name":"stdout","output_type":"stream","text":["Crosslingual posts, posts: 4972, fc: 153743\n"]},{"name":"stderr","output_type":"stream","text":["Processing Batches: 100%|██████████| 311/311 [04:51<00:00,  1.07it/s]\n"]}],"source":["posts = posts_df.loc[tasks['crosslingual']['posts_train']]\n","fc_embeddings = fact_checks_embeddings.loc[tasks['crosslingual']['fact_checks']]\n","\n","print(f\"Crosslingual posts, posts: { len(posts) }, fc: { len(fc_embeddings) }\")\n","\n","positive_samples, negative_samples = get_positive_and_negative_samples(posts, fc_embeddings)\n","\n","positive_samples_indices.update(dict(zip(posts.index.to_list(), positive_samples)))\n","negative_samples_indices.update(dict(zip(posts.index.to_list(), negative_samples)))"]},{"cell_type":"markdown","metadata":{"id":"vyWM6CujuiDX"},"source":["### saving samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVnbbtkAuGhH"},"outputs":[],"source":["folder_path = './data/training data'\n","model_name = 'gte-multilingual-base'\n","lang_type = 'multi'\n","# lang_type = 'eng'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IQQqpW1urtb"},"outputs":[],"source":["# Convert numpy int64 to Python int before serializing\n","def convert_to_int(obj):\n","    if isinstance(obj, np.int64):\n","        return int(obj)\n","    # If it's a dictionary or list, recursively convert values\n","    if isinstance(obj, dict):\n","        return {k: convert_to_int(v) for k, v in obj.items()}\n","    if isinstance(obj, list):\n","        return [convert_to_int(v) for v in obj]\n","    return obj\n","\n","# Now use the converted dictionary with json.dump\n","with open(f'{folder_path}/{model_name}_{lang_type}_positive_samples.json', 'w') as json_file:\n","    json.dump(convert_to_int(positive_samples_indices), json_file, indent=4)\n","\n","with open(f'{folder_path}/{model_name}_{lang_type}_negative_samples.json', 'w') as json_file:\n","    json.dump(convert_to_int(negative_samples_indices), json_file, indent=4)"]},{"cell_type":"markdown","metadata":{"id":"GqWzsSknFWxQ"},"source":["# training the Model"]},{"cell_type":"markdown","metadata":{"id":"VyIaPKPbHxlL"},"source":["## loading and preparing train data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzmG4rt7H2Cr"},"outputs":[],"source":["folder_path = './data/training data'\n","model_name = 'gte-multilingual-base'\n","lang_type = 'multi'\n","\n","with open(f'{folder_path}/{model_name}_{lang_type}_positive_samples.json', 'r') as file:\n","    positive_samples = json.load(file)\n","\n","# with open(f'{folder_path}/{model_name}_{lang_type}_negative_samples.json', 'r') as file:\n","#     negative_samples = json.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ACGpVvBGOzN1"},"outputs":[],"source":["for key, value in positive_samples.items():\n","    positive_samples[key] = fact_checks_df.loc[positive_samples[key]]['claim'].apply(lambda x: x[0]).to_list()\n","\n","# for key, value in negative_samples.items():\n","#     negative_samples[key] = fact_checks_df.loc[negative_samples[key]]['claim'].apply(lambda x: x[0]).to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81-Lgp7IN0U1"},"outputs":[],"source":["posts_train = []\n","posts_train.extend(tasks['crosslingual']['posts_train'])\n","for lang in tasks['monolingual'].keys():\n","    posts_train.extend(tasks['monolingual'][lang]['posts_train'])"]},{"cell_type":"markdown","metadata":{"id":"JoPZsH3eMlDE"},"source":["## train the model"]},{"cell_type":"code","source":["from peft import LoraConfig, get_peft_model\n","\n","config = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA on attention layers\n","    lora_dropout=0.05,\n","    bias=\"none\"\n",")\n","model = get_peft_model(model, config)"],"metadata":{"id":"KrqNolKJKcbU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = []\n","\n","# for post_id in posts_train:\n","for post_id in tasks['monolingual']['tha']['posts_train']:\n","    anchor = posts_df.loc[post_id]['eng_content']\n","    positives = positive_samples[str(post_id)]\n","    # negatives = negative_samples[str(post_id)]\n","\n","    dataset.append({\"anchor\": anchor, \"positives\": positives})"],"metadata":{"id":"N2OHJUAALMLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","from torch.utils.data import Dataset, DataLoader\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=512):\n","        self.data = []\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","        for item in data:\n","            anchor = item[\"anchor\"]\n","            positives = item[\"positives\"]\n","            for positive in positives:\n","                self.data.append((anchor, positive))  # Only use anchor-positive pairs\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        anchor, positive = self.data[idx]\n","\n","        # Tokenize anchor and positive text\n","        encoded = self.tokenizer(\n","            [anchor, positive],\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors=\"pt\",\n","        )\n","\n","        return {\n","            \"anchor\": encoded[\"input_ids\"][0],\n","            \"positive\": encoded[\"input_ids\"][1],\n","            \"anchor_mask\": encoded[\"attention_mask\"][0],\n","            \"positive_mask\": encoded[\"attention_mask\"][1],\n","        }\n","\n","# Create dataset and dataloader\n","train_dataset = CustomDataset(dataset, tokenizer)\n","train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"],"metadata":{"id":"E57i96RVLOaW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ContrastiveLoss(nn.Module):\n","    def __init__(self, temperature=0.05):\n","        super().__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, query_embeddings, pos_embeddings):\n","        # Compute cosine similarities\n","        sim_pos = F.cosine_similarity(query_embeddings, pos_embeddings)\n","\n","        # Contrastive loss (log-softmax over positive and negative pairs)\n","        logits = sim_pos.unsqueeze(1) / self.temperature\n","        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=logits.device)\n","\n","        return F.cross_entropy(logits, labels)"],"metadata":{"id":"y4L7WUlxLRDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","# QLoRA configuration (LoRA with quantized model)\n","from peft import LoraConfig, get_peft_model\n","\n","lora_config = LoraConfig(\n","    r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.1, bias=\"none\"\n",")\n","\n","model = get_peft_model(model, lora_config)\n","\n","# Training parameters\n","epochs = 1\n","batch_size = 4\n","learning_rate = 2e-5\n","\n","# Optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","# Loss function\n","loss_fn = ContrastiveLoss()\n","\n","# DataLoader\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Training loop\n","model.train()\n","\n","for epoch in range(epochs):\n","    total_loss = 0\n","\n","    for batch in tqdm(train_dataloader, desc = \"Batch Processing:\"):\n","        optimizer.zero_grad()\n","\n","        # Move data to GPU\n","        anchor_input = batch[\"anchor\"].to(device)\n","        positive_input = batch[\"positive\"].to(device)\n","\n","        anchor_mask = batch[\"anchor_mask\"].to(device)\n","        positive_mask = batch[\"positive_mask\"].to(device)\n","\n","        # Compute embeddings\n","        anchor_emb = model(input_ids=anchor_input, attention_mask=anchor_mask).last_hidden_state[:, 0]\n","        positive_emb = model(input_ids=positive_input, attention_mask=positive_mask).last_hidden_state[:, 0]\n","\n","        # Compute loss\n","        loss = loss_fn(anchor_emb, positive_emb)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_dataloader):.4f}\")\n","\n","print(\"Training complete!\")"],"metadata":{"id":"4d-1jO00LUWE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8v98osCXR0xv"},"source":["# evaluating the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6AnCt1oSP7z"},"outputs":[],"source":["def get_fact_checks(post_id):\n","    FCs = mapping_df[mapping_df['post_id'] == post_id]['fact_check_id'].to_list()\n","    return fact_checks_df.loc[FCs].index.to_list()\n","\n","def common_element(list1, list2):\n","    return any(item in list2 for item in list1)\n","\n","def get_accuracy(posts_ids, top_indices_ids, show_logs = False):\n","    mismatched_posts = []\n","    corrects = 0\n","    for i, p in enumerate(posts_ids):\n","      FCs = get_fact_checks(p)\n","      result = common_element(FCs, top_indices_ids[i])\n","\n","      if show_logs:\n","        print(\"=================================================================\")\n","        print(f'fact_checks for post {p}')\n","        print(f\"content: {posts_df.loc[p]['content']}\")\n","        print(FCs)\n","        for x in FCs:\n","          print(f\"title: {fact_checks_df.loc[x]['title']}\")\n","          print(f\"claim: {fact_checks_df.loc[x]['claim'][1]}\")\n","        print(result)\n","      corrects += result == True\n","      if not result:\n","        mismatched_posts.append(p)\n","    return corrects, mismatched_posts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2160,"status":"ok","timestamp":1740129847586,"user":{"displayName":"امیر محمد آزادی","userId":"12043806229481158638"},"user_tz":-210},"id":"C15DLn9iR2e0","outputId":"a3c2fce1-c52c-4fc2-e04c-44678efc0b00"},"outputs":[{"output_type":"stream","name":"stderr","text":["Processing Batches: 100%|██████████| 24/24 [00:02<00:00,  9.99it/s]\n"]}],"source":["lang = 'tha'\n","fc = fact_checks_df.loc[tasks['monolingual'][lang]['fact_checks']]\n","# fc = fact_checks_df.loc[tasks['crosslingual']['fact_checks']]\n","\n","# fc_emb = fact_checks_embeddings.loc[fc.index]\n","fc_emb = get_embeddings(fc.index.to_list(), fc['claim'].apply(lambda x: x[0]).to_list(), batch_size = 16)\n","# fc_emb = get_embeddings(fc.index.to_list(), fc['content'].to_list(), batch_size = 32)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ED4chctiSA4j","outputId":"3a9ccd0b-3675-484f-91d8-9ab21270efea","executionInfo":{"status":"ok","timestamp":1740129882528,"user_tz":-210,"elapsed":34951,"user":{"displayName":"امیر محمد آزادی","userId":"12043806229481158638"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["lang: tha, posts: 465, fc: 382\n"]},{"output_type":"stream","name":"stderr","text":["Processing Batches: 100%|██████████| 117/117 [00:34<00:00,  3.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["accuracy: 24.516129032258064% !\n"]}],"source":["# posts = posts_summaries.loc[posts_summaries.index.isin(tasks['monolingual'][lang]['posts_train'])]\n","posts = posts_df.loc[tasks['monolingual'][lang]['posts_train']]\n","# posts = posts_df.loc[tasks['crosslingual']['posts_train']]\n","\n","print(f\"lang: {lang}, posts: { len(posts) }, fc: { len(fc) }\")\n","\n","posts_embedding = get_embeddings(posts.index.to_list(), posts['content'].to_list(), batch_size = 4)\n","\n","# similarities = cosine_similarity(list(posts_embedding.values()), fc_emb['embedding'].to_list())\n","similarities = cosine_similarity(list(posts_embedding.values()), list(fc_emb.values()))\n","\n","nearest = np.argpartition(similarities, -10, axis=1)[:, -10:]\n","# top_indices = [[fc_emb.iloc[idx].name for idx in sublist] for sublist in nearest]\n","top_indices = [[list(fc_emb.keys())[idx] for idx in sublist] for sublist in nearest]\n","\n","corrects, mismatched_posts = get_accuracy(posts.index, top_indices)\n","\n","print(f\"accuracy: {corrects/len(posts) * 100}% !\")"]},{"cell_type":"markdown","metadata":{"id":"pf-1K4WxNFre"},"source":["# saving the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKsP0C3iMUBI"},"outputs":[],"source":["from google.colab import userdata\n","\n","def login2HF():\n","  !huggingface-cli login --token '{userdata.get('HF_token')}'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2048,"status":"ok","timestamp":1740127817154,"user":{"displayName":"AmirMohammad Azadi","userId":"00925377092121741255"},"user_tz":-210},"id":"xgRftS3WNODY","outputId":"bf9bc481-b4f8-4f85-d5e4-c34f99cf656d"},"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","The token `Colab_notebook` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `Colab_notebook`\n"]}],"source":["login2HF()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjEF-0Jzzs1S"},"outputs":[],"source":["model.save_pretrained('gte-multilingual-base_Fine_Tuned_1e')\n","tokenizer.save_pretrained('gte-multilingual-base_Fine_Tuned_1e')\n","!huggingface-cli upload 'gte-multilingual-base_Fine_Tuned_1e'"]}],"metadata":{"colab":{"collapsed_sections":["0MPyaQH7V94y","ewWl3klCZmU9","SyxBvHMKdXW2","jQk9tYC1imXR","v22Irn-WoFKg","RDO-7EpepwWw","7EXS7R7Jpy3k","vyWM6CujuiDX","VyIaPKPbHxlL","8v98osCXR0xv","pf-1K4WxNFre"],"provenance":[{"file_id":"1Z09eHXKtWP8DD97UkNpIV4aEFtKHcbOb","timestamp":1740135003843}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}